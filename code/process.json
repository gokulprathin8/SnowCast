[{
  "id" : "78vedq",
  "name" : "data_sentinel2",
  "description" : null,
  "code" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "mxpyqt",
  "name" : "model_creation_lstm",
  "description" : "python",
  "code" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2xkhz",
  "name" : "model_creation_rf",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass RandomForestHole(BaseHole):\n  \n  def get_model(self):\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    return rfc_pipeline\n\n  def evaluate(self):\n    mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n    mse = metrics.mean_squared_error(self.test_y, self.test_y_results)\n    r2 = metrics.r2_score(self.test_y, self.test_y_results)\n    rmse = math.sqrt(mse)\n\n    print(\"The random forest model performance for testing set\")\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    return {\"mae\":mae, \"mse\": mse, \"r2\": r2, \"rmse\": rmse}",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rauqsh",
  "name" : "model_creation_ghostnet",
  "description" : "python",
  "code" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "mi3e5n",
  "name" : "model_comparison",
  "description" : null,
  "code" : "# Find the best model\nprint(\"model comparison script\")\nprint(\"hello world\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u7xh2p",
  "name" : "data_integration",
  "description" : null,
  "code" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom math import radians\nfrom sklearn import neighbors as sk\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import datetime,timedelta\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nprint(\"integrating datasets into one dataset\")\n# pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\n# example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\nprint(train_labels_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n\n\n# print(station_cell_mapper_pd.head())\n\n# example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n# print(example_mod_pd.shape)\n\n\ndef getDateStr(x):\n    return x.split(\" \")[0]\n\n\ndef integrate_modis():\n    \"\"\"\n  Integrate all MODIS data into mod_all.csv\n  \"\"\"\n    all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n    if os.path.isfile(all_mod_file):\n        return\n    dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n    mod_all_df = pd.DataFrame(columns=[\"date\"])\n    mod_all_df['date'] = dates\n\n    # print(mod_all_df.head())\n    for ind in station_cell_mapper_pd.index:\n        current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n        print(current_cell_id)\n        mod_single_file = f\"{github_dir}/data/sat_training/modis/mod10a1_ndsi_{current_cell_id}.csv\"\n        if os.path.isfile(mod_single_file):\n            mod_single_pd = pd.read_csv(mod_single_file, header=0)\n            mod_single_pd = mod_single_pd[[\"date\", \"mod10a1_ndsi\"]]\n            mod_single_pd = mod_single_pd.rename(columns={\"mod10a1_ndsi\": current_cell_id})\n            mod_single_pd['date'] = pd.to_datetime(mod_single_pd['date']).astype(str)\n            print(mod_all_df.shape)\n            mod_all_df = pd.merge(mod_all_df, mod_single_pd, how='left', on=\"date\")\n    mod_all_df.to_csv(all_mod_file)\n\n\ndef integrate_sentinel1():\n    \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  \"\"\"\n    all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n    if os.path.isfile(all_sentinel1_file):\n        return\n    dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n    sentinel1_all_df = pd.DataFrame(columns=[\"date\"])\n    sentinel1_all_df['date'] = dates\n    # print(mod_all_df.head())\n\n    for ind in station_cell_mapper_pd.index:\n        current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n        print(current_cell_id)\n        sentinel1_single_file = f\"{github_dir}/data/sat_training/sentinel1/s1_grd_vv_{current_cell_id}.csv\"\n        if os.path.isfile(sentinel1_single_file) and current_cell_id not in sentinel1_all_df:\n            sentinel1_single_pd = pd.read_csv(sentinel1_single_file, header=0)\n            sentinel1_single_pd = sentinel1_single_pd[[\"date\", \"s1_grd_vv\"]]\n            sentinel1_single_pd = sentinel1_single_pd.rename(columns={\"s1_grd_vv\": current_cell_id})\n            # sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n            sentinel1_single_pd['date'] = pd.to_datetime(sentinel1_single_pd['date']).dt.round(\"D\").astype(str)\n            print(\"sentinel1_single_pd: \", sentinel1_single_pd.head())\n            print(\"sentinel1_single_pd check value: \", sentinel1_single_pd[sentinel1_single_pd[\"date\"] == \"2015-04-01\"])\n            sentinel1_single_pd = sentinel1_single_pd.drop_duplicates(subset=['date'],\n                                                                      keep='first')  # this will remove all the other values of the same day\n\n            sentinel1_all_df = pd.merge(sentinel1_all_df, sentinel1_single_pd, how='left', on=\"date\")\n            print(\"sentinel1_all_df check value: \", sentinel1_all_df[sentinel1_all_df[\"date\"] == \"2015-04-01\"])\n            print(\"sentinel1_all_df: \", sentinel1_all_df.shape)\n\n    print(sentinel1_all_df.shape)\n    sentinel1_all_df.to_csv(all_sentinel1_file)\n\n\ndef integrate_gridmet():\n    \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n\n    dates = pd.date_range(start='10/1/2018', end='09/30/2019', freq='D').astype(str)\n\n    # print(mod_all_df.head())\n    var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n    for var in var_list:\n        gridmet_all_df = pd.DataFrame(columns=[\"date\"])\n        gridmet_all_df['date'] = dates\n        all_gridmet_file = f\"{github_dir}/data/ready_for_training/gridmet_{var}_all.csv\"\n        if os.path.isfile(all_gridmet_file):\n            return\n        for ind in station_cell_mapper_pd.index:\n            current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n            print(current_cell_id)\n            gridmet_single_file = f\"{github_dir}/data/sim_training/gridmet/{var}_{current_cell_id}.csv\"\n            if os.path.isfile(gridmet_single_file) and current_cell_id not in gridmet_all_df:\n                gridmet_single_pd = pd.read_csv(gridmet_single_file, header=0)\n                gridmet_single_pd = gridmet_single_pd[[\"date\", var]]\n                gridmet_single_pd = gridmet_single_pd.rename(columns={var: current_cell_id})\n                # sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n                gridmet_single_pd['date'] = pd.to_datetime(gridmet_single_pd['date']).dt.round(\"D\").astype(str)\n                print(\"gridmet_single_pd: \", gridmet_single_pd.head())\n                print(\"gridmet_single_pd check value: \", gridmet_single_pd[gridmet_single_pd[\"date\"] == \"2015-04-01\"])\n                gridmet_single_pd = gridmet_single_pd.drop_duplicates(subset=['date'],\n                                                                      keep='first')  # this will remove all the other values of the same day\n\n                gridmet_all_df = pd.merge(gridmet_all_df, gridmet_single_pd, how='left', on=\"date\")\n                print(\"gridmet_all_df check value: \", gridmet_all_df[gridmet_all_df[\"date\"] == \"2015-04-01\"])\n                print(\"gridmet_all_df: \", gridmet_all_df.shape)\n\n        print(gridmet_all_df.shape)\n        gridmet_all_df.to_csv(all_gridmet_file)\n\n\ndef prepare_training_csv():\n    \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_modis_without_nsidc.csv\"\n    if os.path.isfile(all_ready_file):\n        return\n\n    all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n    modis_all_pd = pd.read_csv(all_mod_file, header=0)\n    all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n    sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0)\n    all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n    gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col=0)\n    all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n    gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col=0)\n    all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n    gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col=0)\n    all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n    gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col=0)\n    all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n    gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col=0)\n    all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n    gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col=0)\n    all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n    gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col=0)\n    all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n    gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col=0)\n\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col=1)\n\n    print(\"modis_all_size: \", modis_all_pd.shape)\n    print(\"station size: \", station_cell_mapper_pd.shape)\n    print(\"training_feature_pd size: \", training_feature_pd.shape)\n    print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n\n    all_training_pd = pd.DataFrame(\n        columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"modis_ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\",\n                 \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n    all_training_pd = all_training_pd.reset_index()\n    for index, row in modis_all_pd.iterrows():\n        dt = datetime.strptime(row['date'], '%Y-%m-%d')\n        month = dt.month\n        year = dt.year\n        doy = dt.timetuple().tm_yday\n        print(f\"Dealing {year} {doy}\")\n        for i in range(3, len(row.index)):\n            cell_id = row.index[i][:-2]\n            if cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n                ndsi = row.values[i]\n                swe = train_labels_pd.loc[cell_id, row['date']]\n                grd = sentinel1_all_pd.loc[index, cell_id]\n                eto = gridmet_eto_all_pd.loc[index, cell_id]\n                pr = gridmet_pr_all_pd.loc[index, cell_id]\n                rmax = gridmet_rmax_all_pd.loc[index, cell_id]\n                rmin = gridmet_rmin_all_pd.loc[index, cell_id]\n                tmmn = gridmet_tmmn_all_pd.loc[index, cell_id]\n                tmmx = gridmet_tmmx_all_pd.loc[index, cell_id]\n                vpd = gridmet_vpd_all_pd.loc[index, cell_id]\n                vs = gridmet_vs_all_pd.loc[index, cell_id]\n                lat = grid_terrain_pd.loc[cell_id, \"Longitude [deg]\"]\n                lon = grid_terrain_pd.loc[cell_id, \"Latitude [deg]\"]\n                elevation = grid_terrain_pd.loc[cell_id, \"Elevation [m]\"]\n                aspect = grid_terrain_pd.loc[cell_id, \"Aspect [deg]\"]\n                curvature = grid_terrain_pd.loc[cell_id, \"Curvature [ratio]\"]\n                slope = grid_terrain_pd.loc[cell_id, \"Slope [deg]\"]\n                eastness = grid_terrain_pd.loc[cell_id, \"Eastness [unitCirc.]\"]\n                northness = grid_terrain_pd.loc[cell_id, \"Northness [unitCirc.]\"]\n\n                if not np.isnan(swe):\n                    json_kv = {\"cell_id\": cell_id, \"year\": year, \"m\": month, \"doy\": doy, \"modis_ndsi\": ndsi, \"grd\": grd,\n                               \"eto\": eto,\n                               \"pr\": pr, \"rmax\": rmax, \"rmin\": rmin, \"tmmn\": tmmn, \"tmmx\": tmmx, \"vpd\": vpd, \"vs\": vs,\n                               \"lat\": lat,\n                               \"lon\": lon, \"elevation\": elevation, \"aspect\": aspect, \"curvature\": curvature,\n                               \"slope\": slope,\n                               \"eastness\": eastness, \"northness\": northness, \"swe\": swe}\n                    all_training_pd = all_training_pd.append(json_kv, ignore_index=True)\n\n    print(all_training_pd.shape)\n    all_training_pd.to_csv(all_ready_file)\n\n\ndef loc_closest_gridcell_id(find_lat, find_lon, valid_cols):\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_lat_lon = pd.read_csv(grid_terrain_file, header=0, usecols=['cell_id', 'Latitude [deg]', 'Longitude [deg]']).loc[lambda df: df['cell_id'].isin(valid_cols)]\n    # print(grid_lat_lon.shape)\n    # print(grid_lat_lon)\n    grid_lat_lon_npy = grid_lat_lon.to_numpy()\n    grid_lat_lon_rad = np.array([[radians(x[2]), radians(x[1])] for x in grid_lat_lon_npy])\n    ball_tree = sk.BallTree(grid_lat_lon_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lon))], return_distance=True)\n    # print(dist)\n    print(ind[0][0])\n    print(\"cell id: \", grid_lat_lon.iloc[ind[0][0]]['cell_id'])\n    return ind[0][0], grid_lat_lon.iloc[ind[0][0]]['cell_id']\n\ndef do_grid_cell_clean_up(pd):\n    \"\"\"\n    As the grid cell has _x and _y suffixes which mess everything up. We will do a clean up to remove those duplicated columns.\n    \n    \"\"\"\n    for col in pd.columns:\n      if \"_\" in col:\n        new_col_name = col.split(\"_\")[0]\n        pd.rename(columns={col:new_col_name}, inplace = True)\n    return pd\n    \n\ndef prepare_training_csv_nsidc():\n    \"\"\"\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new_with_modis.csv\"\n    if os.path.isfile(all_ready_file):\n        print(\"The file already exists. Exiting..\")\n        return\n    all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n    gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col=0)\n    all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n    gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col=0)\n    all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n    gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col=0)\n    all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n    gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col=0)\n    all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n    gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col=0)\n    all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n    gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col=0)\n    all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n    gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col=0)\n    all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n    gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col=0)\n    \n    print(\"gridmet_tmmn_all_pd shape: \",gridmet_tmmn_all_pd.shape)\n    \n    all_modis_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n    modis_all_pd = pd.read_csv(all_modis_file, header=0, index_col=0)\n    \n    print(\"modis_all_pd head: \", modis_all_pd.head())\n    print(\"modis_all_pd shape: \", modis_all_pd.shape)\n    modis_all_pd = do_grid_cell_clean_up(modis_all_pd)\n    if \"76b55900-eb3d-4d25-a538-f74302ffe72d\" in modis_all_pd.columns:\n        print(\"76b55900-eb3d-4d25-a538-f74302ffe72d exists in modis_all_pd.\")\n    #print('modis_all_pd[0][\"76b55900-eb3d-4d25-a538-f74302ffe72d\"]: ', modis_all_pd.iloc[0, \"76b55900-eb3d-4d25-a538-f74302ffe72d\"])\n    \n    \n    all_nsidc_file = f\"{github_dir}/data/sim_training/nsidc/2019nsidc_data.csv\"\n    nsidc_all_pd = pd.read_csv(all_nsidc_file, header=0, index_col=0)\n\n    # print(nsidc_all_pd.shape)\n    # print(nsidc_all_pd)\n\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col=0)\n\n    # print(grid_terrain_pd.shape)\n    # print(grid_terrain_pd)\n\n    print(\"station size: \", station_cell_mapper_pd.shape)\n    print(\"training_feature_pd size: \", training_feature_pd.shape)\n    print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n    all_valid_columns = gridmet_eto_all_pd.columns.values\n    all_training_pd = pd.DataFrame(\n        columns=[\"cell_id\", \"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\",\n                 \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe_0719\", \"depth_0719\", \"modis\", \"swe_snotel\"])\n    all_training_pd = all_training_pd.reset_index()\n    for index, row in nsidc_all_pd.iterrows():\n        month = row['Month']\n        year = row['Year']\n        day = row['Day']\n        print(f\"Dealing {year} {month} {day}\")\n        lat = row['Lat']\n        lon = row['Lon']\n        print(\"lat lon: \", lat, \" \", lon)\n        ind, cell_id = loc_closest_gridcell_id(lat, lon, all_valid_columns)\n        swe = row['SWE']\n        depth = row['Depth']\n        index = index % 365\n        eto = gridmet_eto_all_pd.iloc[index][cell_id]\n        pr = gridmet_pr_all_pd.iloc[index][cell_id]\n        rmax = gridmet_rmax_all_pd.iloc[index][cell_id]\n        rmin = gridmet_rmin_all_pd.iloc[index][cell_id]\n        tmmn = gridmet_tmmn_all_pd.iloc[index][cell_id]\n        tmmx = gridmet_tmmx_all_pd.iloc[index][cell_id]\n        vpd = gridmet_vpd_all_pd.iloc[index][cell_id]\n        vs = gridmet_vs_all_pd.iloc[index][cell_id]\n        lat = grid_terrain_pd.loc[ind, \"Latitude [deg]\"]\n        lon = grid_terrain_pd.loc[ind, \"Longitude [deg]\"]\n        elevation = grid_terrain_pd.loc[ind, \"Elevation [m]\"]\n        aspect = grid_terrain_pd.loc[ind, \"Aspect [deg]\"]\n        curvature = grid_terrain_pd.loc[ind, \"Curvature [ratio]\"]\n        slope = grid_terrain_pd.loc[ind, \"Slope [deg]\"]\n        eastness = grid_terrain_pd.loc[ind, \"Eastness [unitCirc.]\"]\n        northness = grid_terrain_pd.loc[ind, \"Northness [unitCirc.]\"]\n        cdate = datetime(year=int(year), month=int(month), day=int(day))\n        current_date = cdate.strftime(\"%Y-%m-%d\")\n        modis = modis_all_pd.iloc[index][cell_id]\n        \n        if cell_id in train_labels_pd.index and current_date in train_labels_pd.columns:\n#           print(\"Check one value: \", train_labels_pd.loc[cell_id][current_date])\n          swe_snotel = train_labels_pd.loc[cell_id][current_date]\n        else:\n          swe_snotel = -1\n#           print(\"Key not existed\")\n\n        if not np.isnan(swe_snotel):\n            json_kv = {\"cell_id\":cell_id,\"year\":year, \"m\":month, \"day\": day, \"eto\":eto, \"pr\":pr, \"rmax\":rmax, \"rmin\":rmin, \"tmmn\":tmmn, \"tmmx\":tmmx, \"vpd\":vpd, \"vs\":vs, \"lat\":lat, \"lon\":lon, \"elevation\":elevation, \"aspect\":aspect, \"curvature\":curvature, \"slope\":slope, \"eastness\":eastness, \"northness\":northness, \"swe_0719\":swe, \"depth_0719\":depth, \"modis\":modis, \"swe_snotel\": swe_snotel}\n            all_training_pd = all_training_pd.append(json_kv, ignore_index=True)\n\n    print(all_training_pd.shape)\n    all_training_pd.to_csv(all_ready_file)\n\n# integrate_modis()\n# integrate_sentinel1()\n# integrate_gridmet()\n# prepare_training_csv()\n\n#prepare_training_csv_nsidc()\nprepare_training_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2wkl6e",
  "name" : "service_deployment",
  "description" : "python",
  "code" : "# Deploy model to service\n\nprint(\"deploy model to service\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "i2fynz",
  "name" : "service_prediction",
  "description" : null,
  "code" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit()  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "e8k4wq",
  "name" : "model_train_validate",
  "description" : null,
  "code" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h1qp9v",
  "name" : "model_predict",
  "description" : null,
  "code" : "# feed testing.csv into ML model\n\ntesting_pd = read_csv(\"final_testing_ready.csv\")\n\nmodel = joblib.load(\"joblib file reload\")\n\nfinal_testing_results = mode.predict(testing_pd)\n\n# match final result values with the original input row's lat/lon\n# use GDAL rasterio or just Python RasterIO package to write to file \n# refer to https://rasterio.readthedocs.io/en/stable/quickstart.html#saving-raster-data\nconvert_result_to_image(final_testing_results, \"final_ml_result_swe_map.tif\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "urd0nk",
  "name" : "data_terrainFeatures",
  "description" : null,
  "code" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\nimport sys, traceback\nimport requests\n\nhome_dir = os.path.expanduser('~')\nsnowcast_github_dir = f\"{home_dir}/Documents/GitHub/SnowCast/\"\n\n#exit() # this process no longer need to execute, we need to make Geoweaver to specify which process doesn't need to run\n\n# user-defined paths for data-access\ndata_dir = f'{snowcast_github_dir}data/'\ngridcells_file = data_dir+'snowcast_provided/grid_cells_eval.geojson'\nstations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\ngridcells_outfile = data_dir+'terrain/gridcells_terrainData_eval.csv'\nstations_outfile = data_dir+'terrain/station_terrainData_eval.csv'\n\nrequests.get('https://planetarycomputer.microsoft.com/api/stac/v1')\n\n# setup client for handshaking and data-access\nprint(\"setup planetary computer client\")\nclient = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",ignore_conformance=True)\n\n# Load metadata\ngridcellsGPD = gpd.read_file(gridcells_file)\ngridcells = geojson.load(open(gridcells_file))\nstations = pd.read_csv(stations_file)\n\n# instantiate output panda dataframes\ndf_gridcells = df = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                            \"Elevation [m]\",\"Aspect [deg]\",\n                                            \"Curvature [ratio]\",\"Slope [deg]\",\n                                            \"Eastness [unitCirc.]\",\"Northness [unitCirc.]\"))\ndf_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                   \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                   \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                   \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                   \"Slope_30 [deg]\",\"Slope_1000 [deg]\",\n                                   \"Eastness_30 [unitCirc.]\",\"Northness_30 [unitCirc.]\",\n                                   \"Eastness_1000 [unitCirc.]\",\"Northness_1000 [unitCirc.]\"))\n\ndef prepareGridCellTerrain():\n  # instantiate output panda dataframes\n  # Calculate gridcell characteristics using Copernicus DEM data\n  print(\"Prepare GridCell Terrain data\")\n  for idx,cell in enumerate(gridcells['features']):\n      print(\"Processing grid \", idx)\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n      )\n      items = list(search.get_items())\n      print(\"==> Searched items: \", len(items))\n\n      cropped_data = None\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              #xarray.open_rasterio(signed_asset.href)\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n      except:\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n\n      # calculate lat/long of center of gridcell\n      longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      print(\"reproject data to EPSG:32612\")\n      # reproject the cropped dem data\n      cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n\n      # Mean elevation of gridcell\n      mean_elev = cropped_data.mean().values\n      print(\"Elevation: \", mean_elev)\n\n      # Calculate directional components\n      aspect = xrspatial.aspect(cropped_data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(\"Aspect: \", mean_aspect)\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n      print(\"Eastness: \", mean_eastness)\n      print(\"Northness: \", mean_northness)\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(cropped_data)\n      mean_curvature = curvature.mean().values\n      print(\"Curvature: \", mean_curvature)\n\n      # Calculate mean slope\n      slope = xrspatial.slope(cropped_data)\n      mean_slope = slope.mean().values\n      print(\"Slope: \", mean_slope)\n\n      # Fill pandas dataframe\n      df_gridcells.loc[idx] = [longitude,latitude,\n                               mean_elev,mean_aspect,\n                               mean_curvature,mean_slope,\n                               mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n      #     df_gridcells.to_csv(gridcells_outfile)\n\n  # Save output data into csv format\n  df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n  df_gridcells.to_csv(gridcells_outfile)\n\ndef prepareStationTerrain():\n  # Calculate terrain characteristics of stations, and surrounding regions using COP 30\n  for idx,station in stations.iterrows():\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Point\", \"coordinates\":[station['longitude'],station['latitude']]},\n      )\n      items = list(search.get_items())\n      print(f\"Returned {len(items)} items\")\n\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n      except:\n          traceback.print_exc(file=sys.stdout)\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n\n      # Reproject the station data to better include only 1000m surrounding area\n      inProj = Proj(init='epsg:4326')\n      outProj = Proj(init='epsg:32612')\n      new_x,new_y = transform(inProj,outProj,station['longitude'],station['latitude'])\n\n      # Calculate elevation of station and surroundings\n      mean_elevation = data.mean().values\n      elevation = data.sel(x=new_x,y=new_y,method='nearest')\n      print(elevation.values)\n\n      # Calcuate directional components\n      aspect = xrspatial.aspect(data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(mean_aspect)\n      aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n      print(aspect.values)\n      eastness = np.cos(aspect*(np.pi/180))\n      northness = np.sin(aspect*(np.pi/180))\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(data)\n      mean_curvature = curvature.mean().values\n      curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n      print(curvature.values)\n\n      # Calculate slope\n      slope = xrspatial.slope(data)\n      mean_slope = slope.mean().values\n      slope = slope.sel(x=new_x,y=new_y,method='nearest')\n      print(slope.values)\n\n      # Fill pandas dataframe\n      df_station.loc[idx] = [station['longitude'],station['latitude'],\n                             station['elevation_m'],elevation.values,mean_elevation,\n                             aspect.values,mean_aspect,\n                             curvature.values,mean_curvature,\n                             slope.values,mean_slope,\n                             eastness.values,northness.values,\n                             mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n      #     df_station.to_csv(stations_outfile)\n\n  # Save output data into CSV format\n  df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n  df_station.to_csv(stations_outfile)\n\ntry:\n  prepareGridCellTerrain()\n  #prepareStationTerrain()\nexcept:\n  traceback.print_exc(file=sys.stdout)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "525l8q",
  "name" : "data_gee_modis_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport eeauth as e\n\n#exit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    # the following is for the server\n    #service_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\n#creds = ee.ServiceAccountCredentials(\n    #service_account, '/home/chetana/bhargavi-creds.json')\n    #ee.Initialize(creds)\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7temiv",
  "name" : "data_gee_sentinel1_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}_{current_cell_id}.csv\"\n\n      if os.path.exists(single_csv_file):\n          print(\"exists skipping..\")\n          continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      viirs = ee.ImageCollection(product_name).filterDate('2013-01-01','2021-12-31').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rmxece",
  "name" : "data_associate_station_grid_cell",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\n\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n\nready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\nresult_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n\nif os.path.exists(result_mapping_file):\n    exit()\n\n\ngridcells = geojson.load(open(gridcells_file))\ntraining_df = pd.read_csv(training_feature_file, header=0)\ntesting_df = pd.read_csv(testing_feature_file, header=0)\nground_measure_metadata_df = pd.read_csv(ground_measure_metadata_file, header=0)\ntrain_labels_df = pd.read_csv(train_labels_file, header=0)\n\nprint(\"training: \", training_df.head())\nprint(\"testing: \", testing_df.head())\nprint(\"ground measure metadata: \", ground_measure_metadata_df.head())\nprint(\"training labels: \", train_labels_df.head())\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n  \n# prepare the training data\n\nstation_cell_mapper_df = pd.DataFrame(columns = [\"station_id\", \"cell_id\", \"lat\", \"lon\"])\n\nground_measure_metadata_df = ground_measure_metadata_df.reset_index()  # make sure indexes pair with number of rows\nfor index, row in ground_measure_metadata_df.iterrows():\n  \t\n    print(row['station_id'], row['name'], row['latitude'], row['longitude'])\n    station_lat = row['latitude']\n    station_lon = row['longitude']\n    \n    shortest_dis = 999\n    associated_cell_id = None\n    associated_lat = None\n    associated_lon = None\n    \n    for idx,cell in enumerate(gridcells['features']):\n    \n      current_cell_id = cell['properties']['cell_id']\n\n      #print(\"collecting \", current_cell_id)\n      cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      dist = calculateDistance(station_lat, station_lon, cell_lat, cell_lon)\n\n      if dist < shortest_dis:\n        associated_cell_id = current_cell_id\n        shortest_dis = dist\n        associated_lat = cell_lat\n        associated_lon = cell_lon\n    \n    station_cell_mapper_df.loc[len(station_cell_mapper_df.index)] = [row['station_id'], associated_cell_id, associated_lat, associated_lon]\n    \nprint(station_cell_mapper_df.head())\nstation_cell_mapper_df.to_csv(f\"{ready_for_training_folder}station_cell_mapping.csv\")\n    \n\n\n      \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "illwc1",
  "name" : "data_gee_modis_real_time",
  "description" : null,
  "code" : "import os\nimport pprint\n\n# import gdal\nimport subprocess\nfrom datetime import datetime, timedelta\n\n# set up your credentials using\n# echo 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\n\nmodis_download_dir = \"/home/chetana/modis_download_folder/\"\nmodis_downloaded_data = modis_download_dir + \"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\ngeo_tiff = modis_download_dir + \"geo-tiff/\"\nvrt_file_dir = modis_download_dir + \"vrt_files/\"\ndir_path = os.path.dirname(os.path.realpath(__file__))\nprint(dir_path)\n\ntile_list = ['h09v04', 'h10v04', 'h11v04', 'h08v04', 'h08v05', 'h09v05', 'h10v05', 'h07v06', 'h08v06', 'h09v06']\n\n\ndef get_files(directory):\n    file_directory = list()\n    complete_directory_structure = dict()\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            file_directory.append(file_path)\n            complete_directory_structure[str(dirpath).rsplit('/')[-1]] = file_directory\n\n    return complete_directory_structure\n\n\ndef get_latest_date():\n    all_rows = get_web_row_data()\n\n    latest_date = None\n    for row in all_rows:\n        try:\n            new_date = datetime.strptime(row.text[:-1], '%Y.%m.%d')\n            if latest_date is None or latest_date < new_date:\n                latest_date = new_date\n        except:\n            continue\n\n    print(\"Find the latest date: \", latest_date.strftime(\"%Y.%m.%d\"))\n    second_latest_date = latest_date - timedelta(days=8)\n    return second_latest_date\n\n\ndef get_web_row_data():\n    try:\n        from BeautifulSoup import BeautifulSoup\n    except ImportError:\n        from bs4 import BeautifulSoup\n    modis_list_url = \"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\n    print(\"Source / Product: \" + modis_list_url)\n    if os.path.exists(\"index.html\"):\n        os.remove(\"index.html\")\n    subprocess.run(\n        f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies '\n        f'--no-check-certificate --auth-no-challenge=on -np -e robots=off {modis_list_url}',\n        shell=True, stderr=subprocess.PIPE)\n    index_file = open('index.html', 'r')\n    webContent = index_file.read()\n    parsed_html = BeautifulSoup(webContent, \"html.parser\")\n    all_rows = parsed_html.body.findAll('td', attrs={'class': 'indexcolname'})\n    return all_rows\n\n\ndef download_recent_modis(date=None):\n    if date:\n        latest_date_str = date.strftime(\"%Y.%m.%d\")\n    else:\n        latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n    for tile in tile_list:\n        download_cmd = f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies ' \\\n                       f'--no-check-certificate --auth-no-challenge=on -r --reject \"i' \\\n                       f'ndex.html*\" -P {modis_download_dir} -np -e robots=off ' \\\n                       f'https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/ -A \"*{tile}*.hdf\" --quiet'\n        # print(download_cmd)\n        p = subprocess.run(download_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        print(\"Downloading tile, \", tile, \" with status code \", \"OK\" if p.returncode == 0 else p.returncode)\n\n\n# def merge_wrap_tif_into_western_us_tif():\n#     latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n#     # traverse the folder and find the new download files\n#     for filename in os.listdir(f\"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/\"):\n#         f = os.path.join(directory, filename)\n#         # checking if it is a file\n#         if os.path.isfile(f):\n#             print(f)\n# merge_wrap_tif_into_western_us_tif()\n\ndef hdf_tif_cvt(resource_path, destination_path):\n    if not os.path.isfile(resource_path):\n        raise Exception(\"HDF file not found\")\n\n    max_snow_extent_path = destination_path + \"maximum_snow_extent/\"\n    eight_day_snow_cover = destination_path + \"eight_day_snow_cover/\"\n    if not os.path.exists(max_snow_extent_path):\n        os.makedirs(max_snow_extent_path)\n    if not os.path.exists(eight_day_snow_cover):\n        os.makedirs(eight_day_snow_cover)\n\n    tif_file_name_snow_extent = max_snow_extent_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_name_eight_day = eight_day_snow_cover + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name_snow_extent + '_max_snow_extent' + tif_file_extension\n    eight_day_snow_cover_file_name = tif_file_name_eight_day + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Maximum_Snow_Extent\"\n    eight_day_snow_cover = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Eight_Day_Snow_Cover\"\n\n    subprocess.run(f\"gdal_translate {maximum_snow_extent} {maximum_snow_extent_file_name}\", shell=True)\n    subprocess.run(f\"gdal_translate {eight_day_snow_cover} {eight_day_snow_cover_file_name}\", shell=True)\n\n\ndef combine_geotiff_gdal(vrt_array, destination):\n    subprocess.run(f\"gdalbuildvrt {destination} {' '.join(vrt_array)}\", shell=True)\n    tif_name = destination.split('.vrt')[-2] + '.tif'\n    subprocess.run(f\"gdal_translate -of GTiff {destination} {tif_name}\", shell=True)\n\n\ndef hdf_tif_conversion(resource_path, destination_path):\n    hdf_dataset = gdal.Open(resource_path)\n    if hdf_dataset is None:\n        raise Exception(\"Could not open HDF dataset\")\n\n    maximum_snow_extent = hdf_dataset.GetSubDatasets()[0][0]\n    modis_snow_500m = hdf_dataset.GetSubDatasets()[1][0]\n\n    driver = gdal.GetDriverByName('GTiff')\n\n    tif_file_name = destination_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name + '_max_snow_extent' + tif_file_extension\n    modis_snow_500m_file_name = tif_file_name + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent_dataset = gdal.Open(maximum_snow_extent)\n    modis_snow_500m_dataset = gdal.Open(modis_snow_500m)\n\n    if maximum_snow_extent_dataset is None:\n        raise Exception(\"Could not open maximum_snow_extent dataset\")\n\n    if modis_snow_500m_dataset is None:\n        raise Exception(\"Could not open modis_snow_500m dataset\")\n\n    driver.CreateCopy(maximum_snow_extent_file_name, maximum_snow_extent_dataset, 0)\n    driver.CreateCopy(modis_snow_500m_file_name, modis_snow_500m_dataset, 0)\n\n    print(\"HDF to TIF conversion completed successfully.\")\n\n\ndef download_modis_archive(*, start_date, end_date):\n    all_archive_dates = list()\n\n    all_rows = get_web_row_data()\n    for r in all_rows:\n        try:\n            all_archive_dates.append(datetime.strptime(r.text.replace('/', ''), '%Y.%m.%d'))\n        except:\n            continue\n\n    for a in all_archive_dates:\n        if start_date <= a <= end_date:\n            download_recent_modis(a)\n\n\ndef step_one_download_modis():\n  download_recent_modis()\n                   \ndef step_two_merge_modis_western_us():\n  download_modis_archive(start_date=datetime(2022, 1, 1), end_date=datetime(2022, 12, 31))\n\n  files = get_files(modis_downloaded_data)\n  for k, v in get_files(modis_downloaded_data).items():\n\n    conversion_path = modis_download_dir + \"geo-tiff/\" + k + \"/\"\n    if not os.path.exists(conversion_path):\n        os.makedirs(conversion_path)\n    for hdf_file in v:\n        # print(hdf_file.split('/')[-1].split('.hdf')[0], 1)\n        hdf_tif_cvt(hdf_file, conversion_path)\n\n  if not os.path.exists(vrt_file_dir):\n    os.makedirs(vrt_file_dir)\n\n\n  directories = [d for d in os.listdir(geo_tiff) if   os.path.isdir(os.path.join(geo_tiff, d))]\n\n  for d in directories:\n    eight_day_snow_cover = geo_tiff + d + '/eight_day_snow_cover'\n    maximum_snow_extent = geo_tiff + d + '/maximum_snow_extent'\n\n    eight_day_abs_path = list()\n    snow_extent_abs_path = list()\n\n    for file in os.listdir(eight_day_snow_cover):\n        file_path = os.path.abspath(os.path.join(eight_day_snow_cover, file))\n        eight_day_abs_path.append(file_path)\n\n    for file in os.listdir(maximum_snow_extent):\n        file_path = os.path.abspath(os.path.join(maximum_snow_extent, file))\n        snow_extent_abs_path.append(file_path)\n\n    combine_geotiff_gdal(eight_day_abs_path, vrt_file_dir + f\"{d}_eight_day.vrt\")\n    combine_geotiff_gdal(snow_extent_abs_path, vrt_file_dir + f\"{d}_snow_extent.vrt\")\n\n                   \n# main workflow is here:\nstep_one_download_modis()\nstep_two_merge_modis_western_us()\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "sjs5by",
  "name" : "data_gee_sentinel1_real_time",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nfrom all_dependencies import *\nfrom snowcast_utils import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nprint(\"submission_format_df shape: \", submission_format_df.shape)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#start_date = \"2022-04-20\"#test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1\",\"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\n\nif os.path.exists(final_csv_file):\n    #print(\"exists skipping..\")\n    #exit()\n    os.remove(final_csv_file)\n\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor current_cell_id in submission_format_df.index:\n  \n    try:\n  \t\n      #print(\"collecting \", current_cell_id)\n      \n      longitude = all_cell_coords_df['lon'][current_cell_id]\n      latitude = all_cell_coords_df['lat'][current_cell_id]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(10)\n\n      viirs = ee.ImageCollection(product_name) \\\n          \t.filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n          \t.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n      \t\t.select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      #print(e)\n      pass\n    \nall_cell_df.to_csv(final_csv_file)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "y7nb46",
  "name" : "base_hole",
  "description" : null,
  "code" : "'''\nThe wrapper for all the snowcast_wormhole predictors\n'''\nimport os\nimport joblib\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass BaseHole:\n  \n  all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new.csv\"\n  \n  def __init__(self):\n    self.classifier = self.get_model()\n    self.holename = self.__class__.__name__ \n    self.train_x = None\n    self.train_y = None\n    self.test_x = None\n    self.test_y = None\n    self.test_y_results = None\n    self.save_file = None\n    \n  def save(self):\n    now = datetime.now()\n    date_time = now.strftime(\"%Y%d%m%H%M%S\")\n    self.save_file = f\"{github_dir}/model/wormhole_{self.holename}_{date_time}.joblib\"\n    print(f\"Saving model to {self.save_file}\")\n    joblib.dump(self.classifier, self.save_file)\n  \n  def preprocessing(self):\n    all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n    input_columns = [\"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \n                     \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \n                     \"northness\", \"modis_ndsi\",\"model_snow_cover\"]\n    \n    all_cols = input_columns\n    all_cols.append(\"swe_snotel\")\n    print(\"all columns: \", all_cols)\n    all_ready_pd = all_ready_pd[all_cols]\n#     all_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\n    all_ready_pd = all_ready_pd[all_ready_pd[\"swe_snotel\"]!=-1]\n    all_ready_pd = all_ready_pd.dropna()\n    train, test = train_test_split(all_ready_pd, test_size=0.2)\n#     \"cell_id\", \"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\",\n#                  \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe_0719\", \"depth_0719\", \"swe_snotel\"\n    self.train_x, self.train_y = train[input_columns].to_numpy().astype('float'), \\\n  \t\t\t\t\t\t\t\ttrain[['swe_snotel']].to_numpy().astype('float')\n    self.test_x, self.test_y = test[input_columns].to_numpy().astype('float'), \\\n    \t\t\t\t\t\t\ttest[['swe_snotel']].to_numpy().astype('float')\n  \n  def train(self):\n    self.classifier.fit(self.train_x, self.train_y)\n  \n  def test(self):\n    self.test_y_results = self.classifier.predict(self.test_x)\n    return self.test_y_results\n  \n  def predict(self, input_x):\n    return self.classifier.predict(input_x)\n  \n  def evaluate(self):\n    pass\n  \n  def get_model(self):\n    pass\n  \n  def post_processing(self):\n    pass",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "a8p3n7",
  "name" : "data_gee_gridmet_station_only",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport os\nimport geopandas as gpd\nimport numpy as np\nimport concurrent.futures\nimport eeauth as e\n\n# authenticate with Earth Engine\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize()\n\n# set up parameters\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\nhomedir = os.path.expanduser('~')\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n\n# helper function to get data for a single cell\ndef get_cell_data(args):\n    cell_id, longitude, latitude = args\n    print(f'Running cell data for lat: {latitude}, long:{longitude}')\n    try:\n        # identify a 500 meter buffer around our Point Of Interest (POI)\n        poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n        viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n        def poi_mean(img):\n            reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n            mean = reducer.get(var_name)\n            return img.set('date', img.date().format()).set(column_name, mean)\n\n        poi_reduced_imgs = viirs.map(poi_mean)\n\n        nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date', column_name]).values().get(0)\n\n        # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n        df = pd.DataFrame(nested_list.getInfo(), columns=['date', column_name])\n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n        df['cell_id'] = cell_id\n        df['latitude'] = latitude\n        df['longitude'] = longitude\n\n        return df\n    except Exception as e:\n        print(e)\n        return None\n\n\n# iterate over variables and cells to retrieve data\nfor var_name in var_list:\n    column_name = var_name\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns=['date', column_name, 'cell_id', 'latitude', 'longitude'])\n    cell_args = [(cell_id, longitude, latitude) for cell_id, longitude, latitude in\n                 zip(station_cell_mapper_df['cell_id'], station_cell_mapper_df['lon'], station_cell_mapper_df['lat'])]\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        cell_dfs = list(executor.map(get_cell_data, cell_args))\n\n    for df in cell_dfs:\n        if df is not None:\n            df_list = [all_cell_df, df]\n            all_cell_df = pd.concat(df_list)  # merge into big dataframe\n\n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "smsdr0",
  "name" : "data_gee_gridmet_real_time",
  "description" : null,
  "code" : "import os\nimport urllib\nimport requests\nfrom bs4 import BeautifulSoup\n\n# download the NetCDF file from Idaho http site daily or the time period matching the MODIS period\n# download site: https://www.northwestknowledge.net/metdata/data/\n\ndownload_source = \"https://www.northwestknowledge.net/metdata/data/\"\ngridmet_download_dir = \"/home/chetana/terrian_data/\"\n\n\ndef download_gridmet():\n    if not os.path.exists(gridmet_download_dir):\n        os.makedirs(gridmet_download_dir)\n\n    soup = BeautifulSoup(requests.get(download_source).text, \"html.parser\")\n    tag_links = soup.find_all('a')\n    for t in tag_links:\n        if '.nc' in t.text and not 'eddi' in t.text and not os.path.isfile(gridmet_download_dir + t.get(\"href\")):\n            print(f'downloading {t.get(\"href\")}')\n            urllib.request.urlretrieve(download_source + t.get('href'), gridmet_download_dir + t.get(\"href\"))\n\n\ndownload_gridmet()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4i0sop",
  "name" : "model_creation_xgboost",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass XGBoostHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b63prf",
  "name" : "testing_data_integration",
  "description" : null,
  "code" : "# integrate all the data into a ML-ready CSV\n\ndef merge_everything_into_csv():\n  \"\"\"\n  read all the downloaded and generated geotiffs for every location and write their values into csv\n  \"\"\"\n  modis_tif_file = \"\"\n  gridmet_nc_file = \"\"\n  terrain_tif_file = \"\"\n  \n  # read them\n  for pixel in modis_tifs:\n    lat = pixel.lat\n    lon = pixel.lon\n    \n    modis_val = modis_tifs.getval(lat, lon)\n    grid_met_val = get_gridmet(lat, lon)\n    terrain_vals = get_terrain(lat, lon)\n    \n    pd.to_csv([modis_val, gridmet_val, terrain_val], \"final_testing_ready.csv\")\n    \nmerge_everything_into_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "zh38b6",
  "name" : "snowcast_utils",
  "description" : null,
  "code" : "from datetime import date\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nimport datetime\n\ntoday = date.today()\n\n# dd/mm/YY\nd1 = today.strftime(\"%Y-%m-%d\")\nprint(\"today date =\", d1)\n\ntrain_start_date = \"\"\ntrain_end_date = \"\"\n\ntest_start_date = \"2022-01-01\"\ntest_end_date = d1\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n\ndef create_cell_location_csv():\n  # read grid cell\n  gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  if os.path.exists(all_cell_coords_file):\n    os.remove(all_cell_coords_file)\n\n  grid_coords_df = pd.DataFrame(columns=[\"cell_id\", \"lat\", \"lon\"])\n  print(grid_coords_df.head())\n  gridcells = geojson.load(open(gridcells_file))\n  for idx,cell in enumerate(gridcells['features']):\n    \n    current_cell_id = cell['properties']['cell_id']\n    cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n    cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n    grid_coords_df.loc[len(grid_coords_df.index)] = [current_cell_id, cell_lat, cell_lon]\n    \n  #grid_coords_np = grid_coords_df.to_numpy()\n  #print(grid_coords_np.shape)\n  grid_coords_df.to_csv(all_cell_coords_file, index=False)\n  #np.savetxt(all_cell_coords_file, grid_coords_np[:, 1:], delimiter=\",\")\n  #print(grid_coords_np.shape)\n  \ndef get_latest_date_from_an_array(arr, date_format):\n  return max(arr, key=lambda x: datetime.datetime.strptime(x, date_format))\n  \n  \ndef findLastStopDate(target_testing_dir, data_format):\n  date_list = []\n  for filename in os.listdir(target_testing_dir):\n    f = os.path.join(target_testing_dir, filename)\n    # checking if it is a file\n    if os.path.isfile(f) and \".csv\" in f:\n        pdf = pd.read_csv(f,header=0, index_col=0)\n        date_list = np.concatenate((date_list, pdf.index.unique()))\n  latest_date = get_latest_date_from_an_array(date_list, data_format)\n  print(latest_date)\n  date_time_obj = datetime.datetime.strptime(latest_date, data_format)\n  return date_time_obj.strftime(\"%Y-%m-%d\")\n\n#create_cell_location_csv()\nfindLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/modis/\", \"%Y-%m-%d\")\n\n\n\n      \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wdh394",
  "name" : "model_create_kehan",
  "description" : null,
  "code" : "\nfrom BaseHole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ilbqzg",
  "name" : "all_dependencies",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n#pd.set_option('display.max_columns', None)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "do86ae",
  "name" : "data_WUS_UCLA_SR",
  "description" : "python",
  "code" : "import os\n\nprint(\"get UCLA data and prepare it into csv\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gkhtc0",
  "name" : "data_nsidc_4km_swe",
  "description" : null,
  "code" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lbd6cp",
  "name" : "model_creation_et",
  "description" : "python",
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass ExtraTreeHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "8ovvln",
  "name" : "data_nsidc_4km_swe_testing",
  "description" : null,
  "code" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\nlat_lon_pairs_rad = []\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()\nfor end_year in range(2001, 2022):\n  start_date = f'{end_year-1}-10-01'\n  end_date = f'{end_year}-09-30'\n  turn_nsidc_nc_to_csv()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "8sv3o6",
  "name" : "gridmet_region_selection_download",
  "description" : "python",
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\nimport multiprocessing\n\n# Initialize GEE\n# try:\n#     ee.Initialize()\n# except Exception as e:\n#     # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n#     ee.Authenticate()\n#     ee.Initialize()\n\nservice_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\ncreds = ee.ServiceAccountCredentials(\n    service_account, '/home/chetana/bhargavi-creds.json')\n\nee.Initialize(creds)\n\n# Read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# Read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(\n    submission_format_file, header=0, index_col=0)\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\nprint(submission_format_df.shape)\n\n# Set the variables\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = findLastStopDate(\n    f\"{github_dir}/data/sim_testing/{org_name}/\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n# Create a list of cell IDs and their coordinates\ncell_coords = [(cell_id, all_cell_coords_pd['lon'][cell_id],\n                all_cell_coords_pd['lat'][cell_id]) for cell_id in submission_format_df.index]\n\n# change this\nstart_date = '2022-01-01'\nend_date = '2022-04-04'\n#\n\n# Define a function to retrieve data for a batch of cells\n\n\ndef get_batch_data(batch):\n    cell_data_list = []\n    for cell_id, longitude, latitude in batch:\n        try:\n            print(f\"=> Collected GridMet data for {cell_id}\")\n            poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n            viirs = ee.ImageCollection(product_name).filterDate(\n                start_date, end_date).filterBounds(poi).select(var_list)\n\n            def poi_mean(img):\n                reducer = img.reduceRegion(\n                    reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n                img = img.set('date', img.date().format())\n                for var in var_list:\n                    column_name = var\n                    mean = reducer.get(column_name)\n                    img = img.set(column_name, mean)\n                return img\n\n            poi_reduced_imgs = viirs.map(poi_mean)\n\n            nested_list = poi_reduced_imgs.reduceColumns(\n                ee.Reducer.toList(var_list), ['date']).get('list')\n\n            df = pd.DataFrame.from_records(nested_list.getInfo(), index='date')\n            df.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n            df['cell_id'] = cell_id\n            df['latitude'] = latitude\n            df['longitude'] = longitude\n\n            cell_data_list.append(df)\n        except Exception as e:\n            print(f\"=> Error collecting data for cell {cell_id}\")\n            print(traceback.format_exc())\n\n    return pd.concat(cell_data_list)\n\n\n# Define a function to collect data for all cells using multiple processes\ndef collect_all_data(cell_coords, num_processes=4):\n    with multiprocessing.Pool(num_processes) as p:\n        results = p.map(get_batch_data, np.array_split(\n            cell_coords, num_processes))\n        return pd.concat(results)\n\n\nnum_processes = 4\ncollect_all_data(cell_coords, num_processes=4)\n\n# try:\n#     print(\"Collecting data for all cells...\")\n#     cell_data = collect_all_data(cell_coords, num_processes=num_processes)\n#     print(\"Data collection complete!\")\n# except Exception as e:\n#     print(f\"Error collecting data for all cells: {e}\")\n#     print(traceback.format_exc())\n# output_dir = f\"{github_dir}/data/sim_testing/{org_name}/\"\n# output_filename = f\"{output_dir}/{start_date.strftime('%Y-%m-%d_%H:%M:%S')}{end_date.strftime('%Y-%m-%d%H:%M:%S')}.csv\"\n\n# try:\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n#         cell_data.to_csv(output_filename)\n#         print(f\"Data saved to {output_filename}\")\n# except Exception as e:\n#     print(f\"Error saving data to {output_filename}: {e}\")\n#     print(traceback.format_exc())\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "11iolq",
  "name" : "gridmet_modis",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\nimport multiprocessing\n\n# Initialize GEE\n# try:\n#     ee.Initialize()\n# except Exception as e:\n#     # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n#     ee.Authenticate()\n#     ee.Initialize()\n\nservice_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\ncreds = ee.ServiceAccountCredentials(\n    service_account, '/home/chetana/bhargavi-creds.json')\n\nee.Initialize(creds)\n\n# Read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# Read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(\n    submission_format_file, header=0, index_col=0)\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\nprint(submission_format_df.shape)\n\n# Set the variables\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = findLastStopDate(\n    f\"{github_dir}/data/sim_testing/{org_name}/\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n# Create a list of cell IDs and their coordinates\ncell_coords = [(cell_id, all_cell_coords_pd['lon'][cell_id],\n                all_cell_coords_pd['lat'][cell_id]) for cell_id in submission_format_df.index]\n\n# change this\nstart_date = '2022-01-01'\nend_date = '2022-04-04'\n#\n\n# Define a function to retrieve data for a batch of cells\n\n\ndef get_batch_data(batch):\n    cell_data_list = []\n    for cell_id, longitude, latitude in batch:\n        try:\n            print(f\"=> Collected GridMet data for {cell_id}\")\n            poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n            viirs = ee.ImageCollection(product_name).filterDate(\n                start_date, end_date).filterBounds(poi).select(var_list)\n\n            def poi_mean(img):\n                reducer = img.reduceRegion(\n                    reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n                img = img.set('date', img.date().format())\n                for var in var_list:\n                    column_name = var\n                    mean = reducer.get(column_name)\n                    img = img.set(column_name, mean)\n                return img\n\n            poi_reduced_imgs = viirs.map(poi_mean)\n\n            nested_list = poi_reduced_imgs.reduceColumns(\n                ee.Reducer.toList(var_list), ['date']).get('list')\n\n            df = pd.DataFrame.from_records(nested_list.getInfo(), index='date')\n            df.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n            df['cell_id'] = cell_id\n            df['latitude'] = latitude\n            df['longitude'] = longitude\n\n            cell_data_list.append(df)\n        except Exception as e:\n            print(f\"=> Error collecting data for cell {cell_id}\")\n            print(traceback.format_exc())\n\n    return pd.concat(cell_data_list)\n\n\n# Define a function to collect data for all cells using multiple processes\ndef collect_all_data(cell_coords, num_processes=4):\n    with multiprocessing.Pool(num_processes) as p:\n        results = p.map(get_batch_data, np.array_split(\n            cell_coords, num_processes))\n        return pd.concat(results)\n\n\nnum_processes = 4\ncollect_all_data(cell_coords, num_processes=4)\n\n# try:\n#     print(\"Collecting data for all cells...\")\n#     cell_data = collect_all_data(cell_coords, num_processes=num_processes)\n#     print(\"Data collection complete!\")\n# except Exception as e:\n#     print(f\"Error collecting data for all cells: {e}\")\n#     print(traceback.format_exc())\n# output_dir = f\"{github_dir}/data/sim_testing/{org_name}/\"\n# output_filename = f\"{output_dir}/{start_date.strftime('%Y-%m-%d_%H:%M:%S')}{end_date.strftime('%Y-%m-%d%H:%M:%S')}.csv\"\n\n# try:\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n#         cell_data.to_csv(output_filename)\n#         print(f\"Data saved to {output_filename}\")\n# except Exception as e:\n#     print(f\"Error saving data to {output_filename}: {e}\")\n#     print(traceback.format_exc())\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "p87wh1",
  "name" : "data_snotel_real_time",
  "description" : null,
  "code" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "oof81y",
  "name" : "ml_post_processing",
  "description" : "python",
  "code" : "# Convert the ML results into SWE map products",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "drd3i0",
  "name" : "data_terrain_imagery",
  "description" : null,
  "code" : "# download the terrain imagery for western U.S. and read values from them\n# north west tile: N48W126\n# north east tile: N49W099\n# south west tile: N26W115\n# south east tile: N26W098\n\n# from azureml.opendatasets import SrtmDownloader\n\nimport csv\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport xarray as xr\nimport pystac_client\nimport planetary_computer\nfrom shapely.wkt import loads\nfrom shapely.geometry import Polygon\n\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\n# select western US as our area of interest\nbbox = [-125.0, 31.0, -102.0, 49.0]\n\narea_of_interest = {\"type\": \"Polygon\", \"coordinates\": [[\n    [bbox[0], bbox[1]],\n    [bbox[2], bbox[1]],\n    [bbox[2], bbox[3]],\n    [bbox[0], bbox[3]],\n    [bbox[0], bbox[1]],\n]]}\n\nsearch = catalog.search(collections=[\"nasadem\"], intersects=area_of_interest)\nwestern_us = search.item_collection()\n\nterrian_file = open('/home/chetana/terrian_downloaded_data/terrian_features.csv', 'w+')\n\nfor tile in western_us:\n    writer = csv.DictWriter(terrian_file, fieldnames=['tile_id', 'lat', 'long', 'northness_30', 'northness_1000',\n                                                      'curvature_30', 'curvature_1000', 'slope_30', 'slope_1000',\n                                                      'elevation_30', 'elevation_1000'])\n    writer.writeheader()\n    coordinates = tile.bbox\n    polygon = str(Polygon([(coordinates[0], coordinates[1]),\n                       (coordinates[2], coordinates[1]),\n                       (coordinates[2], coordinates[3]),\n                       (coordinates[0], coordinates[3])]))\n    polygon = loads(polygon)\n    lat_long = list(polygon.exterior.coords)\n    da = rioxarray.open_rasterio(tile.assets[\"elevation\"].href, variable=\"elevation\").isel(band=0)\n    aspect = xrspatial.aspect(da)\n    aspect_30 = aspect.where((da > (30 - 15)) & (da < (30 + 15)), drop=True)\n    aspect_1000 = aspect.where((da > (1000 - 15) & (1000 + 15)), drop=True)\n    aspect_30_mean = aspect_30.mean()\n    aspect_1000_mean = aspect_1000.mean()\n\n    aspect_30 = aspect_30_mean.values  ##\n    aspect_1000 = aspect_1000_mean.values  ##\n\n    elevation_30_calc = da.where((aspect_30 > 15) & (aspect_30 < 45))\n    elevation_1000_calc = da.where((aspect_1000 > 1000 - 15) & (aspect_1000 < 1000 + 15))\n    elevation_30 = elevation_30_calc.mean().values\n    elevation_1000 = elevation_1000_calc.mean().values\n\n    curvature_30_calc = xrspatial.curvature(da.where((aspect > 30 - 15) & (aspect < 30 + 15)))\n    curvature_30_filter = xr.where((aspect > 30 - 15) & (aspect < 30 + 15), curvature_30_calc, np.nan)\n\n    curvature_1000_calc = xrspatial.curvature(da.where((aspect > 1000 - 15) & (aspect < 1000 + 15)))\n    curvature_1000_filter = xr.where((aspect > 1000 - 15) & (aspect < 1000 + 15), curvature_1000_calc, np.nan)\n\n    curvature_30 = curvature_30_filter.mean().values\n    curvature_1000 = curvature_1000_filter.mean().values\n\n    slope_30_calc = xrspatial.slope(da, 30)\n    slope_1000_calc = xrspatial.slope(da, 1000)\n\n    slope_30 = slope_30_calc.mean().item()\n    slope_1000 = slope_1000_calc.mean().item()\n\n    northness_30_calc = xrspatial.aspect(slope_30_calc)\n    northness_1000_calc = xrspatial.aspect(slope_1000_calc)\n\n    northness_30 = northness_30_calc.mean().item()\n    northness_1000 = northness_1000_calc.mean().item()\n\n    for l in lat_long:\n        print(l[0], l[1])\n        row = {'tile_id': tile.id, 'lat': l[1], 'long': l[0], 'northness_30': northness_30, 'northness_1000': northness_1000,\n               'curvature_30': curvature_30, 'curvature_1000': curvature_1000, 'slope_30': slope_30,\n               'slope_1000': slope_1000, 'elevation_30': elevation_30, 'elevation_1000': elevation_1000}\n        writer.writerow(row)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
