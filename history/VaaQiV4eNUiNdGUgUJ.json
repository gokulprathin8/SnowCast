[{
  "history_id" : "i190n3atjgo",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684617,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9johq7747yk",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684631,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "yg042nsd5cf",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684640,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "rgs74qb0m4y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684647,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "pizuzvd7d1j",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684651,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "6q4kjbez5dd",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684657,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "fwoz43itog1",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684666,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ooy6rhaqwll",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684679,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "bof9i5t6upg",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684687,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "nwwelut1bx0",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684698,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "llmb5aoofo5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684707,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "2fnkd1me7sz",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684717,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "16qfhmioonj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684724,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "p6bfxql53cs",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\n\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n\nready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\nresult_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n\nif os.path.exists(result_mapping_file):\n    exit()\n\n\ngridcells = geojson.load(open(gridcells_file))\ntraining_df = pd.read_csv(training_feature_file, header=0)\ntesting_df = pd.read_csv(testing_feature_file, header=0)\nground_measure_metadata_df = pd.read_csv(ground_measure_metadata_file, header=0)\ntrain_labels_df = pd.read_csv(train_labels_file, header=0)\n\nprint(\"training: \", training_df.head())\nprint(\"testing: \", testing_df.head())\nprint(\"ground measure metadata: \", ground_measure_metadata_df.head())\nprint(\"training labels: \", train_labels_df.head())\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n  \n# prepare the training data\n\nstation_cell_mapper_df = pd.DataFrame(columns = [\"station_id\", \"cell_id\", \"lat\", \"lon\"])\n\nground_measure_metadata_df = ground_measure_metadata_df.reset_index()  # make sure indexes pair with number of rows\nfor index, row in ground_measure_metadata_df.iterrows():\n  \t\n    print(row['station_id'], row['name'], row['latitude'], row['longitude'])\n    station_lat = row['latitude']\n    station_lon = row['longitude']\n    \n    shortest_dis = 999\n    associated_cell_id = None\n    associated_lat = None\n    associated_lon = None\n    \n    for idx,cell in enumerate(gridcells['features']):\n    \n      current_cell_id = cell['properties']['cell_id']\n\n      #print(\"collecting \", current_cell_id)\n      cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      dist = calculateDistance(station_lat, station_lon, cell_lat, cell_lon)\n\n      if dist < shortest_dis:\n        associated_cell_id = current_cell_id\n        shortest_dis = dist\n        associated_lat = cell_lat\n        associated_lon = cell_lon\n    \n    station_cell_mapper_df.loc[len(station_cell_mapper_df.index)] = [row['station_id'], associated_cell_id, associated_lat, associated_lon]\n    \nprint(station_cell_mapper_df.head())\nstation_cell_mapper_df.to_csv(f\"{ready_for_training_folder}station_cell_mapping.csv\")\n    \n\n\n      \n",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1678756683312,
  "history_end_time" : 1678756694989,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "hcojqy6c60i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683130,
  "history_end_time" : 1678756684749,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7zp65zotvyh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683156,
  "history_end_time" : 1678756684752,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "alubxyr833e",
  "history_input" : "'''\nThe wrapper for all the snowcast_wormhole predictors\n'''\nimport os\nimport joblib\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass BaseHole:\n  \n  all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new.csv\"\n  \n  def __init__(self):\n    self.classifier = self.get_model()\n    self.holename = self.__class__.__name__ \n    self.train_x = None\n    self.train_y = None\n    self.test_x = None\n    self.test_y = None\n    self.test_y_results = None\n    self.save_file = None\n    \n  def save(self):\n    now = datetime.now()\n    date_time = now.strftime(\"%Y%d%m%H%M%S\")\n    self.save_file = f\"{github_dir}/model/wormhole_{self.holename}_{date_time}.joblib\"\n    print(f\"Saving model to {self.save_file}\")\n    joblib.dump(self.classifier, self.save_file)\n  \n  def preprocessing(self):\n    all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n    input_columns = [\"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe_0719\", \"depth_0719\"]\n    \n    all_cols = input_columns\n    all_cols.append(\"swe_snotel\")\n    print(\"all columns: \", all_cols)\n    print(type(i) for i in all_cols)\n    all_ready_pd = all_ready_pd[all_cols]\n#     all_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\n    all_ready_pd = all_ready_pd[all_ready_pd[\"swe_snotel\"]!=-1]\n    all_ready_pd = all_ready_pd.dropna()\n    print(\"all ready df columns used for traing: \",all_ready_pd.columns)\n    print(\"all ready df columns shape: \",all_ready_pd.shape)\n    train, test = train_test_split(all_ready_pd, test_size=0.2)\n#     \"cell_id\", \"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\",\"vpd\", \"vs\", \"lat\", \"lon\",\n#                  \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe_0719\", \"depth_0719\", \"swe_snotel\"\n    print(\"Training data columns: \",train.columns)\n    self.train_x, self.train_y = train[input_columns].to_numpy().astype('float'), train[['swe_snotel']].to_numpy().astype('float')\n    self.test_x, self.test_y = test[input_columns].to_numpy().astype('float'), test[['swe_snotel']].to_numpy().astype('float')\n  \n  def train(self):\n    self.classifier.fit(self.train_x, self.train_y)\n  \n  def test(self):\n    self.test_y_results = self.classifier.predict(self.test_x)\n    return self.test_y_results\n  \n  def predict(self, input_x):\n    return self.classifier.predict(input_x)\n  \n  def evaluate(self):\n    pass\n  \n  def get_model(self):\n    pass\n  \n  def post_processing(self):\n    pass",
  "history_output" : "",
  "history_begin_time" : 1678756683389,
  "history_end_time" : 1678756694583,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "wmwansidi9v",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684766,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "stkd509vod2",
  "history_input" : "\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\nimport eeauth as e\n\n#exit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\nprint(submission_format_df.shape)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\n#start_date = \"2022-04-20\"#test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sim_testing/{org_name}/\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n#start_date = \"2022-04-06\"\n#end_date = \"2022-04-18\"\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_testing/{org_name}/\"\nif not os.path.exists(dfolder):\n  os.makedirs(dfolder)\n  \ncolumn_list = ['date', 'cell_id', 'latitude', 'longitude']\ncolumn_list.extend(var_list)\nreduced_column_list = ['date']\nreduced_column_list.extend(var_list)\n\nall_cell_df = pd.DataFrame(columns = column_list)\n\n\ncount = 0\n\nfor current_cell_id in submission_format_df.index:\n\n  try:\n    count+=1\n    #print(f\"=> Collected GridMet data for {count} cells\") #uncomment to print\n    #print(\"collecting \", current_cell_id) #uncomment to print\n    #single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n    #if os.path.exists(single_csv_file):\n    #  os.remove(single_csv_file)\n    #  print(\"exists skipping..\")\n    #  continue\n\n    longitude = all_cell_coords_pd['lon'][current_cell_id]\n    latitude = all_cell_coords_pd['lat'][current_cell_id]\n\n    # identify a 500 meter buffer around our Point Of Interest (POI)\n    poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n    viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_list)\n\n    def poi_mean(img):\n      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n      img = img.set('date', img.date().format());\n      for var in var_list:\n        column_name = var\n        mean = reducer.get(column_name)\n        img = img.set(column_name,mean)\n      return img\n\n\n    poi_reduced_imgs = viirs.map(poi_mean)\n\n    nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(9), reduced_column_list).values().get(0)\n\n    # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n    df = pd.DataFrame(nested_list.getInfo(), columns=reduced_column_list)\n\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index('date')\n\n    df['cell_id'] = current_cell_id\n    df['latitude'] = latitude\n    df['longitude'] = longitude\n    #df.to_csv(single_csv_file)\n\n    #print(df.head())\n    \n    df_list = [all_cell_df, df]\n    all_cell_df = pd.concat(df_list) # merge into big dataframe\n    \n    #print(all_cell_df)\n    #if count % 4 == 0:\n\n  except Exception as e:\n    print(traceback.format_exc())\n    print(\"Failed: \", e)\n    pass\n\nall_cell_df.to_csv(f\"{dfolder}/all_vars_{start_date}_{end_date}.csv\")  \nprint('DONE')\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1678756683911,
  "history_end_time" : 1678756694595,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "g7yjucmjfd9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684783,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "9vkhict013r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683220,
  "history_end_time" : 1678756684788,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4xygbh50fsy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683230,
  "history_end_time" : 1678756684789,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "4xz37rsi7x4",
  "history_input" : "\nfrom base_hole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "history_output" : "Running",
  "history_begin_time" : 1678756689002,
  "history_end_time" : 1678756694578,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "lyha1gx4d4p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683255,
  "history_end_time" : 1678756684806,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "336qv0qzigj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683265,
  "history_end_time" : 1678756684809,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "lc7cvgwoeqt",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684812,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "7olidaf6507",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684823,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "ilag9pn5tin",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678756684831,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
},{
  "history_id" : "vjsry0r527p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678756683316,
  "history_end_time" : 1678756684837,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Stopped"
}]
